### 現状と課題
水鳥のデータセットでDreamer の学習を行い、画像の潜在表現を獲得できるか実験した。その際、224×224の入力に対応したエンコーダ、デコーダを新たに実装し、静止画像分類故に行動や報酬を設定しない1シーケンスのエピソードに対応するようにRSSMを設計し直したりした。 一応学習は完了できたのですが、学習過程で得られた.npz ファイルに含まれる潜在表現から復元した画像は一面グレー1色で正常に学習できていない。

### まず結論

- **Dreamer 系ワールドモデルでも「分類用の潜在表現」を学習することは可能** ですが、
    
    - (i) **ピクセル再構成がきちんと機能していること**、
        
    - (ii) **KL divergence と再構成損失のバランスが適切であること**、
        
    - (iii) **単画像（長さ 1 シーケンス）という特殊条件**に合わせてハイパーパラメータと前処理を調整していること、  
        が前提になります。
        
- 出力が “真っ灰⾊（127 付近）” になるのは **典型的な VAE／Dreamer の「表現崩壊（posterior collapse or decoder collapse）」**。ほとんどの場合は **前処理・損失関数・KL フリー ナッツ設定の不整合** が原因です。([Stack Overflow](https://stackoverflow.com/questions/64909658/what-could-cause-a-vaevariational-autoencoder-to-output-random-noise-even-afte "python - What could cause a VAE(Variational AutoEncoder) to output random noise even after training? - Stack Overflow"))
    
- **RL パート（行動・価値・報酬）がゼロ** でも、Dreamer は “VAE＋1 ステップ予測” として動くので理論上は問題ありません。ただし「シーケンス長 1」だと **遷移モデルに学習信号が入らない** ため、ほぼ純粋な VAE とみなすのが実態です。
    

---

## 1. 原因を絞り込むチェックリスト

|項目|ありがちな落とし穴|確認ポイント|
|---|---|---|
|**データ前処理**|_入力_ を `[-1, 1]` で正規化、_Decoder_ 出力を `sigmoid → [0, 1]` にしている（レンジ不一致）|学習データと再構成画像の分布を `histogram` で比較|
|**損失関数**|Dreamer の `logprob_image` は _discretized logistic mixture_ を想定（0-255 int）。自前実装で **MSE** に置き換えた場合、重みを適切に下げないと KL が勝つ → 平均灰色に収束|`loss_image` と `loss_kl` の比率を mlflow でプロット|
|**KL Free Nats / β**|`kl_free_nats = 3.0` などが**大きすぎ** → KL ≈ 0、Decoder 無関心|`entropy_prior - entropy_post` が 0 付近で張り付いていないか|
|**Decoder 容量不足**|224×224 に対し conv 層が Dreamer 標準 (48×48 想定) のまま|Layer 出力サイズを `summary()` で確認；ResNet-like decoderへ置換も検討|
|**BatchNorm の無効化**|高分解能入力で BN が効かず勾配爆発・消失|`std` が 0 付近に落ちていないか|
|**遷移モデル未学習**|シーケンス長 1 で `recurrent_state` が更新されず、Prior が常に平均化|Trainer で `open_loop_context = 0` にしても挙動が変わらないか|

---

## 2. すぐに試せるデバッグ手順

1. **前処理を Dreamer 準拠に統一**
    
    ```python
    # 0–255 uint8 -> float32 in [-0.5, +0.5]
    img = (img.astype(np.float32) / 255.) - 0.5
    decoder_out = torch.tanh(x)  # same range
    loss = 255. * F.mse_loss(decoder_out + 0.5, target)  # if you stay with MSE
    ```
    
2. **`kl_free_nats` を 0 → 1 → 3 とスイープ**  
    異常に再構成が灰色なら _まず 0_ で試し、KL と再構成の勾配量を観察。
    
3. **解像度を 128×128 に下げて再学習**  
    224×224 用に conv チャネルを 64→128 と増やすより早いフィルタリング手段。
    
4. **`muDreamer` 的 _reconstruction-free_ 訓練を試す**  
    MuDreamer は “画像再構成しない Dreamer” で表現崩壊を回避した事例。([OpenReview](https://openreview.net/forum?id=9pe38WpsbX&utm_source=chatgpt.com "MuDreamer: Learning Predictive World Models without Reconstruction"))
    
    - カメラ入力 → latent
        
    - latent から **前ステップ Value / Action** を予測して学習  
        → Decoder 不要なので “灰色崩壊” 自体が起こらない。
        
5. **t-SNE／UMAP で latent を可視化**  
    クラス分離が全く無ければ、Reconstruction 問題を直しても分類への転移は難しい。
    

---

## 3. Dreamer を「静止画分類」に使う設計のポイント

|設計箇所|推奨|
|---|---|
|**入力シーケンス**|同一画像を _k_ 回（例: 8 ステップ）リピートし、最後に “ダミー行動＝NOP” を与える。こうすると RSSM の遷移モデルにも勾配が流れる。|
|**報酬**|Phase 2 以降で **概念忠実度 (TCAV Score) を即時報酬** として与える。シーケンス長>1 にすれば _行動=概念編集_ が定義可能。|
|**表現学習ロス**|Reconstruction だけでなく **SimCLR／Barlow Twins などの contrastive loss** を latent に並列適用すると分類性能が安定。|
|**Decoder**|224×224 を本気で再構成したい場合は _U-Net Lite_（skip-connection 付き）か _ViT Decoder_ を別途実装する方がパラメータ効率が良い。|

---

## 4. もし “Dreamer だと遠回り” と感じたら

- **純粋な VAE + SimCLR** で latent を作る → RSSM 遷移モデルを後付け（Phase 2）
    
- あるいは **Contextual RSSM** や **HRSSM**（階層表現）を流用すると、高分解能でも安定しやすい。([OpenReview](https://openreview.net/pdf?id=8BJl6LQgW5&utm_source=chatgpt.com "[PDF] VISUAL REPRESENTATION LEARNING FOR WORLD MODELS BY ..."))
    

---

### まとめ

- **灰色画像 → まずは再構成ループの正常化（レンジ＆KL バランス）を確認。**
    
- **Dreamer ＝ RL 専用ではなく “予測モデル + 価値モデル” の枠組み**。分類タスクでも VAE 的に使えます。
    
- **シーケンス長 1 では “ただの VAE” になる** ので、遷移モデルを生かすなら同一画像の複写やダミー行動注入で長さを稼ぐ or reconstruction-free へ舵を切るのが王道です。
    

これらを踏まえてハイパーパラメータを再調整し、まずは **128×128 / KL=0 で再構成がカラーになるか** を確認してみてください。